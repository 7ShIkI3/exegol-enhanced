#!/usr/bin/env python3

"""
Enhanced Web Fuzzer - Exegol Enhanced
Developed with AI assistance for comprehensive web application testing
Version: 1.0.0

Features:
- Directory and file fuzzing
- Parameter fuzzing
- Custom wordlists
- Multi-threading
- Response analysis
- Output in multiple formats
"""

import argparse
import asyncio
import aiohttp
import time
import sys
import os
import json
import csv
from urllib.parse import urljoin, urlparse
from pathlib import Path
import random
import string
from datetime import datetime
import signal

# Colors for terminal output
class Colors:
    RED = '\033[91m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    BLUE = '\033[94m'
    PURPLE = '\033[95m'
    CYAN = '\033[96m'
    WHITE = '\033[97m'
    BOLD = '\033[1m'
    END = '\033[0m'

class WebFuzzer:
    def __init__(self, args):
        self.target_url = args.url
        self.wordlist_path = args.wordlist
        self.threads = args.threads
        self.delay = args.delay
        self.timeout = args.timeout
        self.output_dir = args.output
        self.extensions = args.extensions.split(',') if args.extensions else []
        self.status_codes = [int(x) for x in args.status_codes.split(',')]
        self.user_agent = args.user_agent
        self.headers = {}
        self.cookies = {}
        self.proxy = args.proxy
        self.verbose = args.verbose
        self.mode = args.mode
        self.parameters = args.parameters.split(',') if args.parameters else []
        
        # Results storage
        self.results = []
        self.total_requests = 0
        self.successful_requests = 0
        self.start_time = None
        
        # Session management
        self.session = None
        self.running = True
        
        # Setup signal handler for graceful shutdown
        signal.signal(signal.SIGINT, self.signal_handler)
        
        # Create output directory
        Path(self.output_dir).mkdir(parents=True, exist_ok=True)
        
        # Parse additional headers
        if args.headers:
            for header in args.headers.split(','):
                if ':' in header:
                    key, value = header.split(':', 1)
                    self.headers[key.strip()] = value.strip()
        
        # Parse cookies
        if args.cookies:
            for cookie in args.cookies.split(','):
                if '=' in cookie:
                    key, value = cookie.split('=', 1)
                    self.cookies[key.strip()] = value.strip()

    def signal_handler(self, signum, frame):
        """Handle Ctrl+C gracefully"""
        self.print_info("Received interrupt signal. Shutting down gracefully...")
        self.running = False

    def print_banner(self):
        """Print application banner"""
        banner = f"""
{Colors.PURPLE}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    ENHANCED WEB FUZZER                      â•‘
â•‘                     Exegol Enhanced                         â•‘
â•‘                                                              â•‘
â•‘              ðŸŒ Advanced Web Application Testing ðŸŒ         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{Colors.END}
"""
        print(banner)

    def print_info(self, message):
        print(f"{Colors.BLUE}[INFO]{Colors.END} {message}")

    def print_success(self, message):
        print(f"{Colors.GREEN}[SUCCESS]{Colors.END} {message}")

    def print_warning(self, message):
        print(f"{Colors.YELLOW}[WARNING]{Colors.END} {message}")

    def print_error(self, message):
        print(f"{Colors.RED}[ERROR]{Colors.END} {message}")

    def print_found(self, status_code, url, size, response_time):
        """Print found results with color coding"""
        if status_code == 200:
            color = Colors.GREEN
        elif status_code in [301, 302, 307, 308]:
            color = Colors.YELLOW
        elif status_code == 403:
            color = Colors.RED
        else:
            color = Colors.CYAN
        
        print(f"{color}[{status_code}]{Colors.END} {url} (Size: {size}, Time: {response_time:.2f}s)")

    def load_wordlist(self):
        """Load wordlist from file"""
        try:
            with open(self.wordlist_path, 'r', encoding='utf-8', errors='ignore') as f:
                wordlist = [line.strip() for line in f if line.strip()]
            
            self.print_success(f"Loaded {len(wordlist)} words from wordlist")
            return wordlist
        except FileNotFoundError:
            self.print_error(f"Wordlist file not found: {self.wordlist_path}")
            sys.exit(1)
        except Exception as e:
            self.print_error(f"Error loading wordlist: {e}")
            sys.exit(1)

    def generate_urls(self, wordlist):
        """Generate URLs based on mode and wordlist"""
        urls = []
        
        if self.mode == 'directory':
            # Directory fuzzing
            for word in wordlist:
                # Base directory
                urls.append(urljoin(self.target_url, word + '/'))
                
                # With extensions if specified
                for ext in self.extensions:
                    urls.append(urljoin(self.target_url, word + ext))
                    
        elif self.mode == 'file':
            # File fuzzing
            for word in wordlist:
                # Base file
                urls.append(urljoin(self.target_url, word))
                
                # With extensions
                for ext in self.extensions:
                    urls.append(urljoin(self.target_url, word + ext))
                    
        elif self.mode == 'parameter':
            # Parameter fuzzing
            base_url = self.target_url
            if '?' not in base_url:
                base_url += '?'
            else:
                base_url += '&'
                
            for param in self.parameters:
                for word in wordlist:
                    urls.append(f"{base_url}{param}={word}")
        
        return urls

    async def create_session(self):
        """Create aiohttp session with configuration"""
        connector = aiohttp.TCPConnector(
            limit=self.threads,
            limit_per_host=self.threads,
            ttl_dns_cache=300,
            use_dns_cache=True,
        )
        
        timeout = aiohttp.ClientTimeout(total=self.timeout)
        
        # Setup headers
        headers = {
            'User-Agent': self.user_agent,
            **self.headers
        }
        
        # Setup proxy if specified
        proxy_url = None
        if self.proxy:
            proxy_url = self.proxy
        
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers=headers,
            cookies=self.cookies,
            trust_env=True
        )

    async def make_request(self, url):
        """Make HTTP request and analyze response"""
        if not self.running:
            return None
            
        try:
            start_time = time.time()
            
            async with self.session.get(
                url,
                proxy=self.proxy,
                allow_redirects=False,
                ssl=False  # Ignore SSL errors for testing
            ) as response:
                content = await response.read()
                response_time = time.time() - start_time
                
                self.total_requests += 1
                
                # Check if status code is in our target list
                if response.status in self.status_codes:
                    self.successful_requests += 1
                    
                    result = {
                        'url': url,
                        'status_code': response.status,
                        'content_length': len(content),
                        'response_time': response_time,
                        'headers': dict(response.headers),
                        'timestamp': datetime.now().isoformat()
                    }
                    
                    # Analyze content type
                    content_type = response.headers.get('content-type', '').lower()
                    result['content_type'] = content_type
                    
                    # Check for interesting content
                    if content:
                        content_str = content.decode('utf-8', errors='ignore').lower()
                        result['interesting'] = self.analyze_content(content_str)
                    
                    self.results.append(result)
                    
                    # Print result
                    if self.verbose or response.status in [200, 301, 302, 403]:
                        self.print_found(
                            response.status,
                            url,
                            len(content),
                            response_time
                        )
                    
                    return result
                    
        except asyncio.TimeoutError:
            if self.verbose:
                self.print_warning(f"Timeout: {url}")
        except aiohttp.ClientError as e:
            if self.verbose:
                self.print_warning(f"Client error for {url}: {e}")
        except Exception as e:
            if self.verbose:
                self.print_error(f"Unexpected error for {url}: {e}")
        
        return None

    def analyze_content(self, content):
        """Analyze response content for interesting patterns"""
        interesting_patterns = [
            'admin', 'login', 'password', 'config', 'database',
            'backup', 'test', 'debug', 'error', 'exception',
            'sql', 'mysql', 'postgresql', 'mongodb',
            'api', 'json', 'xml', 'swagger',
            'git', 'svn', 'cvs',
            'phpinfo', 'server-status', 'server-info'
        ]
        
        found_patterns = []
        for pattern in interesting_patterns:
            if pattern in content:
                found_patterns.append(pattern)
        
        return found_patterns

    async def fuzzer_worker(self, url_queue):
        """Worker coroutine for processing URLs"""
        while not url_queue.empty() and self.running:
            try:
                url = await url_queue.get()
                await self.make_request(url)
                
                # Add delay if specified
                if self.delay > 0:
                    await asyncio.sleep(self.delay)
                    
                url_queue.task_done()
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                if self.verbose:
                    self.print_error(f"Worker error: {e}")

    async def run_fuzzer(self):
        """Main fuzzer execution"""
        self.print_info(f"Target: {self.target_url}")
        self.print_info(f"Mode: {self.mode}")
        self.print_info(f"Threads: {self.threads}")
        self.print_info(f"Status codes: {self.status_codes}")
        
        # Load wordlist
        wordlist = self.load_wordlist()
        
        # Generate URLs
        urls = self.generate_urls(wordlist)
        self.print_info(f"Generated {len(urls)} URLs to test")
        
        # Create URL queue
        url_queue = asyncio.Queue()
        for url in urls:
            await url_queue.put(url)
        
        # Create session
        await self.create_session()
        
        # Start timing
        self.start_time = time.time()
        
        # Create and start workers
        workers = []
        for _ in range(self.threads):
            worker = asyncio.create_task(self.fuzzer_worker(url_queue))
            workers.append(worker)
        
        # Progress reporting
        progress_task = asyncio.create_task(self.progress_reporter(len(urls)))
        
        # Wait for completion
        await url_queue.join()
        
        # Cancel workers and progress reporter
        for worker in workers:
            worker.cancel()
        progress_task.cancel()
        
        # Close session
        await self.session.close()
        
        # Generate reports
        await self.generate_reports()

    async def progress_reporter(self, total_urls):
        """Report progress periodically"""
        while self.running:
            try:
                await asyncio.sleep(10)  # Report every 10 seconds
                
                elapsed = time.time() - self.start_time
                rate = self.total_requests / elapsed if elapsed > 0 else 0
                
                self.print_info(
                    f"Progress: {self.total_requests}/{total_urls} "
                    f"({self.total_requests/total_urls*100:.1f}%) "
                    f"- Found: {self.successful_requests} "
                    f"- Rate: {rate:.1f} req/s"
                )
                
            except asyncio.CancelledError:
                break

    async def generate_reports(self):
        """Generate output reports"""
        if not self.results:
            self.print_warning("No results to report")
            return
        
        self.print_info("Generating reports...")
        
        # Sort results by status code and URL
        self.results.sort(key=lambda x: (x['status_code'], x['url']))
        
        # Generate JSON report
        json_file = os.path.join(self.output_dir, 'fuzzer_results.json')
        with open(json_file, 'w') as f:
            json.dump({
                'scan_info': {
                    'target': self.target_url,
                    'mode': self.mode,
                    'start_time': self.start_time,
                    'total_requests': self.total_requests,
                    'successful_requests': self.successful_requests,
                    'duration': time.time() - self.start_time
                },
                'results': self.results
            }, f, indent=2)
        
        # Generate CSV report
        csv_file = os.path.join(self.output_dir, 'fuzzer_results.csv')
        with open(csv_file, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['URL', 'Status Code', 'Content Length', 'Response Time', 'Content Type', 'Interesting'])
            
            for result in self.results:
                writer.writerow([
                    result['url'],
                    result['status_code'],
                    result['content_length'],
                    f"{result['response_time']:.2f}",
                    result['content_type'],
                    ','.join(result.get('interesting', []))
                ])
        
        # Generate text summary
        summary_file = os.path.join(self.output_dir, 'fuzzer_summary.txt')
        with open(summary_file, 'w') as f:
            f.write("=== EXEGOL ENHANCED WEB FUZZER SUMMARY ===\n")
            f.write(f"Scan Date: {datetime.now()}\n")
            f.write(f"Target: {self.target_url}\n")
            f.write(f"Mode: {self.mode}\n")
            f.write(f"Total Requests: {self.total_requests}\n")
            f.write(f"Successful Requests: {self.successful_requests}\n")
            f.write(f"Duration: {time.time() - self.start_time:.2f}s\n\n")
            
            # Group by status code
            status_groups = {}
            for result in self.results:
                status = result['status_code']
                if status not in status_groups:
                    status_groups[status] = []
                status_groups[status].append(result)
            
            for status_code in sorted(status_groups.keys()):
                f.write(f"=== STATUS CODE {status_code} ({len(status_groups[status_code])} results) ===\n")
                for result in status_groups[status_code]:
                    f.write(f"{result['url']} (Size: {result['content_length']}, Time: {result['response_time']:.2f}s)\n")
                f.write("\n")
        
        self.print_success(f"Reports generated in: {self.output_dir}")
        self.print_info(f"  - JSON: {json_file}")
        self.print_info(f"  - CSV: {csv_file}")
        self.print_info(f"  - Summary: {summary_file}")

def create_default_wordlist():
    """Create a default wordlist if none provided"""
    default_words = [
        'admin', 'administrator', 'login', 'test', 'backup', 'config',
        'database', 'db', 'sql', 'mysql', 'api', 'v1', 'v2', 'dev',
        'staging', 'prod', 'production', 'www', 'mail', 'email',
        'ftp', 'ssh', 'telnet', 'smtp', 'pop', 'imap', 'dns',
        'ldap', 'ad', 'directory', 'users', 'accounts', 'passwords',
        'secret', 'private', 'public', 'upload', 'download', 'files',
        'documents', 'images', 'videos', 'audio', 'media', 'assets',
        'static', 'css', 'js', 'javascript', 'jquery', 'bootstrap',
        'php', 'asp', 'aspx', 'jsp', 'cgi', 'perl', 'python',
        'ruby', 'node', 'java', 'dotnet', 'framework', 'lib',
        'library', 'vendor', 'third-party', 'external', 'internal'
    ]
    
    return default_words

def main():
    parser = argparse.ArgumentParser(
        description='Enhanced Web Fuzzer - Exegol Enhanced',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  %(prog)s -u http://example.com -w /usr/share/wordlists/dirb/common.txt
  %(prog)s -u http://example.com -m file -e .php,.html,.txt
  %(prog)s -u http://example.com -m parameter -p id,name,search
  %(prog)s -u http://example.com -t 50 -d 0.1 --status-codes 200,403,500
        """
    )
    
    # Required arguments
    parser.add_argument('-u', '--url', required=True,
                       help='Target URL (e.g., http://example.com/)')
    
    # Optional arguments
    parser.add_argument('-w', '--wordlist', default='',
                       help='Wordlist file path')
    parser.add_argument('-m', '--mode', choices=['directory', 'file', 'parameter'],
                       default='directory', help='Fuzzing mode (default: directory)')
    parser.add_argument('-t', '--threads', type=int, default=20,
                       help='Number of threads (default: 20)')
    parser.add_argument('-d', '--delay', type=float, default=0,
                       help='Delay between requests in seconds (default: 0)')
    parser.add_argument('--timeout', type=int, default=10,
                       help='Request timeout in seconds (default: 10)')
    parser.add_argument('-o', '--output', default='./fuzzer_output',
                       help='Output directory (default: ./fuzzer_output)')
    parser.add_argument('-e', '--extensions', default='',
                       help='File extensions to test (comma-separated)')
    parser.add_argument('-s', '--status-codes', default='200,301,302,403,500',
                       help='Status codes to report (default: 200,301,302,403,500)')
    parser.add_argument('-p', '--parameters', default='',
                       help='Parameters to fuzz (comma-separated, for parameter mode)')
    parser.add_argument('--headers', default='',
                       help='Custom headers (comma-separated, format: "Header:Value")')
    parser.add_argument('--cookies', default='',
                       help='Custom cookies (comma-separated, format: "name=value")')
    parser.add_argument('--user-agent', default='ExegolEnhanced-WebFuzzer/1.0',
                       help='Custom User-Agent string')
    parser.add_argument('--proxy', default='',
                       help='Proxy URL (e.g., http://127.0.0.1:8080)')
    parser.add_argument('-v', '--verbose', action='store_true',
                       help='Verbose output')
    
    args = parser.parse_args()
    
    # Validate URL
    parsed_url = urlparse(args.url)
    if not parsed_url.scheme or not parsed_url.netloc:
        print(f"{Colors.RED}[ERROR]{Colors.END} Invalid URL: {args.url}")
        sys.exit(1)
    
    # Handle wordlist
    if not args.wordlist:
        print(f"{Colors.YELLOW}[WARNING]{Colors.END} No wordlist specified, using default wordlist")
        # Create temporary wordlist file
        import tempfile
        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt') as f:
            for word in create_default_wordlist():
                f.write(word + '\n')
            args.wordlist = f.name
    
    # Create fuzzer instance
    fuzzer = WebFuzzer(args)
    
    # Print banner
    fuzzer.print_banner()
    
    try:
        # Run the fuzzer
        asyncio.run(fuzzer.run_fuzzer())
        
        # Print final summary
        elapsed = time.time() - fuzzer.start_time
        rate = fuzzer.total_requests / elapsed if elapsed > 0 else 0
        
        print(f"\n{Colors.GREEN}ðŸŽ‰ Fuzzing completed!{Colors.END}")
        print(f"{Colors.CYAN}Total Requests: {fuzzer.total_requests}{Colors.END}")
        print(f"{Colors.CYAN}Successful Requests: {fuzzer.successful_requests}{Colors.END}")
        print(f"{Colors.CYAN}Duration: {elapsed:.2f}s{Colors.END}")
        print(f"{Colors.CYAN}Average Rate: {rate:.1f} req/s{Colors.END}")
        
    except KeyboardInterrupt:
        print(f"\n{Colors.YELLOW}Fuzzing interrupted by user{Colors.END}")
    except Exception as e:
        print(f"\n{Colors.RED}Fuzzing failed: {e}{Colors.END}")
        sys.exit(1)

if __name__ == '__main__':
    main()
